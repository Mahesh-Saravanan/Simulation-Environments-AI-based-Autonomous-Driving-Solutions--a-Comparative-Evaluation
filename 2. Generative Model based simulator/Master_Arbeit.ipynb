{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Dependencies"
      ],
      "metadata": {
        "id": "daVYrx16dlDI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2fTsnjFQjaQB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4911d90-eb30-49ff-e859-ef5fcf4f3531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model,load_model\n",
        "from tensorflow.keras.layers import Dense, Flatten,Lambda,Reshape,Dropout,BatchNormalization,Concatenate,ReLU\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Layer\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading datapoints"
      ],
      "metadata": {
        "id": "1gHXF9uVOICa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- $m$ - Observation at previous timestep $O_{t}$\n",
        "- $a$ - Action label (3d vector)\n",
        "- $p$ - Observation at next timestep $O_{t+1}$\n",
        "\n",
        "Each file contains ~1000 datapoints"
      ],
      "metadata": {
        "id": "dhbfOGmWMZO6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YwsAZ_kuW4r0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3079de1-7395-4612-bc51-ce5ecf9b4c97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading Data --> [===================================================================================================> ] 99%"
          ]
        }
      ],
      "source": [
        "m = np.load('/content/drive/MyDrive/Thesis/plan Sai/Dataset/Dataset 2 - Different Values/m_ob0.npy')\n",
        "a = np.load('/content/drive/MyDrive/Thesis/plan Sai/Dataset/Dataset 2 - Different Values/act0.npy')\n",
        "p = np.load('/content/drive/MyDrive/Thesis/plan Sai/Dataset/Dataset 2 - Different Values/p_ob0.npy')\n",
        "\n",
        "for i in range(1,123):\n",
        "  # Progress Bar\n",
        "  progress = i / 123;bar_length = 100;completed_length = int(bar_length * progress);bar = \"Reading Data --> [\" + \"=\" * completed_length + \">\" + \" \" * (bar_length - completed_length) + \"]\";percentage = f\"{progress * 100:.0f}%\";sys.stdout.write(f\"\\r{bar} {percentage}\");sys.stdout.flush()\n",
        "\n",
        "  #Loading and concatnating each file\n",
        "  m= np.concatenate((m,np.load(f'/content/drive/MyDrive/Thesis/plan Sai/Dataset/Dataset 2 - Different Values/m_ob{i}.npy')),axis = 0)\n",
        "  a= np.concatenate((a,np.load(f'/content/drive/MyDrive/Thesis/plan Sai/Dataset/Dataset 2 - Different Values/act{i}.npy')),axis = 0)\n",
        "  p= np.concatenate((p,np.load(f'/content/drive/MyDrive/Thesis/plan Sai/Dataset/Dataset 2 - Different Values/p_ob{i}.npy')),axis = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "uhEkPUS5P2Tg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YMT4IETSjVVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9bfd2c-c614-46df-e95f-4da0303fce2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Datapoints: 130068\n",
            "Total Batches: 64\n"
          ]
        }
      ],
      "source": [
        "#Clipping the values between 0 to 1\n",
        "m[m > 1] = 1\n",
        "p[p > 1] = 1\n",
        "num_actions = 3\n",
        "\n",
        "#Coupling m and a as a pair\n",
        "m_with_action = np.column_stack((m,a))\n",
        "m_dim = m.shape[1]\n",
        "action_map = {0:\"Straight\",1:\"Left\",2:\"Right\"}\n",
        "\n",
        "\n",
        "# Converting Numpy arrays to tensor datatype\n",
        "m_t = tf.data.Dataset.from_tensor_slices(m_with_action)\n",
        "p_t = tf.data.Dataset.from_tensor_slices(p)\n",
        "dataset = tf.data.Dataset.zip((m_t, p_t))\n",
        "\n",
        "def cast_to_new_dtype(element1, element2):return tf.dtypes.cast(element1, tf.float32), tf.dtypes.cast(element2, tf.float32)\n",
        "dataset = dataset.map(cast_to_new_dtype)\n",
        "\n",
        "print(f\"Total Datapoints: {dataset.cardinality().numpy()}\")\n",
        "shuffled_dataset = dataset.shuffle(100)\n",
        "dataset = shuffled_dataset.batch(2046)\n",
        "print(f\"Total Batches: {dataset.cardinality().numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Plot function"
      ],
      "metadata": {
        "id": "Gw49aqRzQacq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3wdbRx9ojvPt"
      },
      "outputs": [],
      "source": [
        "def plot(x,y,generated = [],generated_a = False):\n",
        "  if len(generated) != 0: generated_a = True\n",
        "  try:\n",
        "    x = x.numpy().reshape(363,)\n",
        "    y = y.numpy().reshape(360,)\n",
        "    if generated_a:generated = generated.numpy();generated = generated.reshape(360,)\n",
        "  except:pass\n",
        "  munnadi = x[:360]\n",
        "  pinnadi = y[:360]\n",
        "  action__ = action_map[x[-3]]\n",
        "  turn_angle = x[-2]\n",
        "  velocity = x[-1]\n",
        "  print(f\"Action: {action__}, Turn Angle: {turn_angle}, Velocity: {velocity}\")\n",
        "  if generated_a: print(f\"Deviation: {MeanSquaredError()(y,generated)}\")\n",
        "  fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(8, 4))\n",
        "  ax1.plot(munnadi, label='Before')\n",
        "  ax1.legend()\n",
        "  ax1.set_yticks([])\n",
        "  ax2.plot(pinnadi, label='After')\n",
        "  if generated_a: ax2.plot(generated, label = \"Generated\")\n",
        "  ax2.legend()\n",
        "  ax2.set_yticks([])\n",
        "  plt.subplots_adjust(hspace=0)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ploting a sample datapoint"
      ],
      "metadata": {
        "id": "ZmM04QATRWN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in dataset.take(1):plot(x[0],y[0])"
      ],
      "metadata": {
        "id": "OuN3jPiNQiyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building GAN..."
      ],
      "metadata": {
        "id": "u7VeNbkndO5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`class NormalRandomLayer` - A sampling layer.\n",
        "\n",
        "`class GAN` - Class with components of GAN\n",
        "- &nbsp; `build_generator()` - Returns the Generator model (Encoder-Decoder Architecture)\n",
        "\n",
        "- &nbsp; `generator_loss()` - **Args** - Discriminator Output (Real/Fake), Generator Output ( $Og_{t+1}$), Target Data ( $O_{t+1}$)\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **To Do** -\n",
        "\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.   Compute Cross entropy loss between 1s and Discriminator Output\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.   Compute MSE between  $Og_{t+1}$ and $O_{t+1}$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Return** - Total Loss, Crossentopy Loss, Mean Square Loss, Weighted Loss\n",
        "\n",
        "- &nbsp; `build_discriminator()` - Returns the Discriminator model\n",
        "\n",
        "- &nbsp; `discriminator_loss()` - **Args** - Generator Output ( $Og_{t+1}$), Target Data ( $O_{t+1}$)\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **To Do** - Compare the Generated data with 0 (*fake*) and Target Data with 1(*Real*)\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Return** - Cross entropy Loss\n",
        "\n",
        "- &nbsp; `compile()` - Compiles Both Generator and discriminator with Loss functions and Optimizer\n",
        "\n",
        "- &nbsp; `visual_inspection()` - Samples a random data point from the batch and feed forwards through generator\n",
        "\n",
        "- &nbsp; `execute()` - Excutes Custom action command on given Observation $O_{t}$ and returns the Generated  $Og_{t+1}$\n",
        "\n",
        "`class Monitor` - Monitors the Performance at end of every `take_action_epoch` epoch\n"
      ],
      "metadata": {
        "id": "5fJ5FuIxRg_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SVnXBaakVW1g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZvBhAG5zIB5y"
      },
      "outputs": [],
      "source": [
        "class NormalRandomLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(NormalRandomLayer, self).__init__(**kwargs)\n",
        "        self.dimension = 512\n",
        "    def call(self, inputs):\n",
        "        shape = (tf.shape(inputs)[0], self.dimension)  # Use the batch size of the input\n",
        "        random_numbers = tf.random.normal(shape)\n",
        "        return random_numbers\n",
        "\n",
        "class GAN(Model):\n",
        "  def __init__(self,m_dim = 360, num_actions = 3, Load_model = False, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.m_dim = m_dim\n",
        "    self.num_actions = num_actions\n",
        "    self.sample_layer = True\n",
        "    self.generator = self.build_generator()\n",
        "    self.discriminator = self.build_discriminator()\n",
        "    if Load_model:\n",
        "      self.generator = load_model(\"/content/drive/MyDrive/Thesis/plan Sai/Models/Generator.h5\")\n",
        "      self.discriminator = load_model(\"/content/drive/MyDrive/Thesis/plan Sai/Models/Discriminator.h5\")\n",
        "    self.l1_lamda = 100\n",
        "    self.action_map = {0:\"Straight\",1:\"Left\",2:\"Right\"}\n",
        "\n",
        "  def build_generator(self):\n",
        "\n",
        "    #________________________________________________________________Input Layer____________________________________________________________________\n",
        "    input_layer = keras.Input(shape=(m_dim+num_actions,))\n",
        "    # spliting the input layer to Observation and action\n",
        "    split_layer = Lambda(lambda x: (x[:, :m_dim], x[:, m_dim:]))\n",
        "    observation,action = split_layer(input_layer)\n",
        "\n",
        "    action = Dense(16)(action)\n",
        "    #________________________________________________________________Encoder____________________________________________________________________\n",
        "    #Concatnating the action label with every layer of Encoder with a dropout and a ReLU layer\n",
        "    e1 = Dense(256)(observation)\n",
        "    e1 = Concatenate()([e1,action])\n",
        "    e1 = Dropout(0.5)(e1)\n",
        "    e1 = ReLU()(e1)\n",
        "\n",
        "    e2 = Dense(128)(e1)\n",
        "    e2 = Concatenate()([e2,action])\n",
        "    e2 = Dropout(0.5)(e2)\n",
        "    e2 = ReLU()(e2)\n",
        "\n",
        "\n",
        "    e3 = Dense(64)(e2)\n",
        "    e3 = Concatenate()([e3,action])\n",
        "    e3 = Dropout(0.5)(e3)\n",
        "    e3 = ReLU()(e3)\n",
        "\n",
        "    e4 = Dense(32)(e3)\n",
        "    e4 = Concatenate()([e4,action])\n",
        "    e4 = Dropout(0.5)(e4)\n",
        "    e4 = ReLU()(e4)\n",
        "    #________________________________________________________________Latent____________________________________________________________________\n",
        "    latent = Dense(16)(e4)\n",
        "\n",
        "    if self.sample_layer:\n",
        "      #Sampling from normal distribution\n",
        "      sample = NormalRandomLayer()(observation)\n",
        "      # Gate\n",
        "      sample = Concatenate()([sample,latent])\n",
        "      sample = Dense(256, activation = \"sigmoid\")(sample)\n",
        "      latent = Concatenate()([latent,sample])\n",
        "    #________________________________________________________________Decoder____________________________________________________________________\n",
        "    #Concatnating the action label and encoder output with every layer of decoder with a dropout and a ReLU layer\n",
        "    d4 =  Dense(32)(latent)\n",
        "    d4 = Concatenate()([d4,e4])\n",
        "    d4 = Concatenate()([d4,action])\n",
        "    d4 = Dropout(0.5)(d4)\n",
        "    d4 = ReLU()(d4)\n",
        "\n",
        "    d3 =  Dense(64)(d4)\n",
        "    d3 = Concatenate()([d3,e3])\n",
        "    d3 = Concatenate()([d3,action])\n",
        "    d3 = Dropout(0.5)(d3)\n",
        "    d3 = ReLU()(d3)\n",
        "\n",
        "    d2 =  Dense(128)(d3)\n",
        "    d2 = Concatenate()([d2,e2])\n",
        "    d2 = Concatenate()([d2,action])\n",
        "    d2 = Dropout(0.5)(d2)\n",
        "    d2 = ReLU()(d2)\n",
        "\n",
        "    d1 =  Dense(256)(d2)\n",
        "    d1 = Concatenate()([d1,e1])\n",
        "    d1 = Concatenate()([d1,action])\n",
        "    d1 = Dropout(0.5)(d1)\n",
        "    d1 = ReLU()(d1)\n",
        "    #________________________________________________________________Output Layer____________________________________________________________________\n",
        "    output_layer = Dense(m_dim,activation = 'sigmoid')(d1)\n",
        "\n",
        "    model =  Model(inputs=input_layer, outputs=output_layer, name=\"generator\")\n",
        "    return model\n",
        "\n",
        "  def generator_loss(self,disc_out,gen_out,target):\n",
        "    reality_loss = self.cross_entropy(tf.ones_like(disc_out), disc_out) # Discriminator Loss\n",
        "    l1_loss = tf.keras.losses.MeanSquaredError()(target , gen_out) # Mean square Error\n",
        "\n",
        "    total_loss = reality_loss + (self.l1_lamda * l1_loss) # Weighted Loss\n",
        "\n",
        "    gen_weighted_loss = (self.l1_lamda * l1_loss) # For Monitoring Purpose\n",
        "    return total_loss, reality_loss, l1_loss, gen_weighted_loss\n",
        "\n",
        "\n",
        "  def build_discriminator(self):\n",
        "\n",
        "    input_layer_p = keras.Input(shape=(m_dim,)) # Output of Generator\n",
        "\n",
        "    #Inputs of Generator\n",
        "    input_layer_m = keras.Input(shape=(m_dim+num_actions,))\n",
        "\n",
        "    #Spliting Observation and Action Label\n",
        "    split_layer = Lambda(lambda x: (x[:, :m_dim], x[:, m_dim:]))\n",
        "    observation,action = split_layer(input_layer_m)\n",
        "\n",
        "    action = Dense(16)(action)\n",
        "\n",
        "    x = Concatenate()([input_layer_m,input_layer_p])\n",
        "    x = Dense(256)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "\n",
        "    x = Dense(128)(x)\n",
        "    x = Concatenate()([x,action])\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Dense(64)(x)\n",
        "    x = Concatenate()([x,action])\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Dense(32)(x)\n",
        "    x = Concatenate()([x,action])\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Dense(8)(x)\n",
        "    x = Concatenate()([x,action])\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    output_layer = Dense(1,activation = 'sigmoid')(x)\n",
        "    model =  Model(inputs=[input_layer_m,input_layer_p], outputs=output_layer, name=\"discriminator\")\n",
        "    return model\n",
        "\n",
        "  def discriminator_loss(self,target, generated):\n",
        "    real_loss = self.cross_entropy(tf.ones_like(target), target)\n",
        "    generated_loss = self.cross_entropy(tf.zeros_like(generated), generated)\n",
        "    total_disc_loss = real_loss + generated_loss\n",
        "    return total_disc_loss\n",
        "\n",
        "  def compile(self):\n",
        "    super().compile()\n",
        "    self.l1 = lambda x, y:  tf.reduce_mean(tf.abs(x - y))\n",
        "    self.cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "    self.generator_optimizer = Adam(2e-3, beta_1=0.5)\n",
        "    self.discriminator_optimizer = Adam(2e-3, beta_1=0.5)\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self, x):\n",
        "    # x[0] ==> Observation Before\n",
        "    # x[1] ==> Observation After\n",
        "\n",
        "    #______________________________________________________Training Generator_____________________________________________________________________________________\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "      gen_output = self.generator(x[0], training=True)\n",
        "      disc_output_on_generated_data = self.discriminator([x[0], gen_output], training=False)\n",
        "      gen_total_loss, gen_gan_loss, gen_l1_loss,gen_weighted_loss = self.generator_loss(disc_output_on_generated_data, gen_output, x[1])\n",
        "    generator_gradients = gen_tape.gradient(gen_total_loss, self.generator.trainable_variables)\n",
        "    self.generator_optimizer.apply_gradients(zip(generator_gradients, self.generator.trainable_variables))\n",
        "\n",
        "    #_____________________________________________________Training Discriminator_____________________________________________________________________________________\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "      gen_output = self.generator(x[0], training=False)\n",
        "      disc_output_on_generated_data = self.discriminator([x[0], gen_output], training=True)\n",
        "      disc_output_on_target_data = self.discriminator([x[0], x[1]], training=True)\n",
        "      disc_loss = self.discriminator_loss(disc_output_on_target_data, disc_output_on_generated_data)\n",
        "    discriminator_gradients = disc_tape.gradient(disc_loss,self.discriminator.trainable_variables)\n",
        "    self.discriminator_optimizer.apply_gradients(zip(discriminator_gradients,self.discriminator.trainable_variables))\n",
        "    #___________________________________________________________________________________________________________________________________________\n",
        "\n",
        "\n",
        "\n",
        "    display = {\"Gen_total_loss\":gen_total_loss,\n",
        "               \"Disc_loss\":disc_loss,\n",
        "               \"Reality_loss\":gen_gan_loss,\n",
        "               \"L1 loss\":gen_l1_loss,\n",
        "               \"Weighted L1 Loss\": gen_weighted_loss}\n",
        "    return display\n",
        "\n",
        "  def visual_inspection(self,x,y):\n",
        "    m = np.random.randint(0,x.shape[0]-1)\n",
        "    f1 = x[m]\n",
        "    f2 = y[m]\n",
        "    f3 = self.generator(x)[m]\n",
        "    plot(f1,f2,generated = f3)\n",
        "\n",
        "  def execute(self,observation,turn_angle,velocity):\n",
        "    action = 0.\n",
        "    if turn_angle>0: action = 1.\n",
        "    elif turn_angle<0: action = 2.\n",
        "    inp = np.array(list(observation)[:360]+[action,turn_angle,velocity])\n",
        "    inp = inp.reshape(1,363)\n",
        "    pred = self.generator(inp)\n",
        "    pred = pred.numpy().reshape(360,)\n",
        "    plot (inp[0],pred)\n",
        "\n",
        "class Monitor(Callback):\n",
        "  def __init__(self,dataset,take_action_epoch = 5):\n",
        "    self.data = dataset\n",
        "    self.take_action_epoch = take_action_epoch\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    if epoch%self.take_action_epoch == 0:\n",
        "      samples = self.data.shuffle(buffer_size=self.data.cardinality().numpy())\n",
        "      for x, y in samples.take(1): cgan.visual_inspection(x,y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Composite model..."
      ],
      "metadata": {
        "id": "EODuEvvnbnjO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1T6vtz0RmFqm"
      },
      "outputs": [],
      "source": [
        "cgan = GAN(num_actions = num_actions,Load_model = False)\n",
        "cgan.compile()\n",
        "monitor = Monitor(dataset,take_action_epoch = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train..."
      ],
      "metadata": {
        "id": "sBKqvWwiAVVO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-ZkGwegJEL5"
      },
      "outputs": [],
      "source": [
        "cgan.fit(dataset, epochs =50,  callbacks = [monitor])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "Br-wZzoHdC-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing with True Labels"
      ],
      "metadata": {
        "id": "1RKkpbyIAKX6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CQD1duamRyt"
      },
      "outputs": [],
      "source": [
        "for x,y in dataset.take(1): cgan.visual_inspection(x,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Sampling a random observation for seed (Initial Observation)"
      ],
      "metadata": {
        "id": "0t0Wxkxi_9Ku"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "h4AGLe7qCrXn"
      },
      "outputs": [],
      "source": [
        "shuf = dataset.shuffle(buffer_size=10).take(1)\n",
        "for x,_ in shuf: observation =x [0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Action Command"
      ],
      "metadata": {
        "id": "GC_AA8Gk__04"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5grWN3V-o_9"
      },
      "outputs": [],
      "source": [
        "turn_angle = 35\n",
        "velocity = 1.0\n",
        "cgan.execute(observation,turn_angle/60,velocity)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Loop"
      ],
      "metadata": {
        "id": "C-dNI7LqADgo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-R0yj8sWB1R"
      },
      "outputs": [],
      "source": [
        "plt.plot(list(observation)[:-3])\n",
        "plt.show()\n",
        "for _ in range (10):\n",
        "  turn_angle = np.random.uniform(-1, 1)\n",
        "  velocity = np.random.uniform(-1, 1)\n",
        "  action = 0.\n",
        "  if turn_angle>0: action = 1.\n",
        "  elif turn_angle<0: action = 2.\n",
        "  inp = np.array(list(observation)[:360]+[action,turn_angle,velocity])\n",
        "  inp = inp.reshape(1,363)\n",
        "  pred = cgan.generator(inp)\n",
        "  pred = pred.numpy().reshape(360,)\n",
        "  observation = pred\n",
        "  plt.plot(list(observation))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Models"
      ],
      "metadata": {
        "id": "j193P4Xa_xG8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAhkZMgeGT2I"
      },
      "outputs": [],
      "source": [
        "cgan.generator.save(\"/content/drive/MyDrive/Thesis/plan Sai/Models/Generator.h5\")\n",
        "cgan.discriminator.save(\"/content/drive/MyDrive/Thesis/plan Sai/Models/Discriminator.h5\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "daVYrx16dlDI",
        "1gHXF9uVOICa",
        "uhEkPUS5P2Tg",
        "Gw49aqRzQacq",
        "u7VeNbkndO5r",
        "EODuEvvnbnjO"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}